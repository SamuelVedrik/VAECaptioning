{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Introduction\n",
    "#### Variational Autoencoder for Deep Learning of Images, Labels and Captions\n",
    "\n",
    "This notebook aims to document the process of implementing [this paper](https://proceedings.neurips.cc/paper/2016/file/eb86d510361fc23b59f18c1bc9802cc6-Paper.pdf) from scratch.\n",
    "I used [this article](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73) to help me get a better intution of what varitional autoencoders do. I recently learnt about autoencoders from my dimensionality reduction class in CSC311 offered by UofT, however they just gave us a very brief introduction to them. Nevertheless, the intutions of the varitional autoencoder makes sense to me. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "#### Convolution Review\n",
    "\n",
    "I was quite rusty with how traditional convolutions networks worked, and so I had to do a quick review. [This article](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53) was a great help for getting me to re-understand it much quicker. \n",
    "\n",
    "Diving deep into \"convolution\" and \"cross correlation\" lead into a lot of signal processing which I am unfamiliar with. I feel that all I need to know is that convolution performs cross correlation with flipped kernels. Pytorch performs cross correlation, so I won't have to flip kernels here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import itertools\n",
    "import torch.nn.init as init\n",
    "import math\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "a_test = np.array([[[1, 2, 0, 0],\n",
    "               [5, 3, 0, 4],\n",
    "               [0, 0, 0, 7],\n",
    "               [9, 3, 0, 0]], \n",
    "              \n",
    "              [[7, 2, 2, 0],\n",
    "               [4, 3, 0, 1],\n",
    "               [0, 5, 0, 0],\n",
    "               [0, 2, 1, 1]]])\n",
    "\n",
    "k_test = np.array([[[1,1,1],[1,1,0],[1,0,0]], [[1,1,1],[1,0,1],[1,1,1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 24, 11,  3],\n",
       "       [25, 31, 18,  6],\n",
       "       [22, 27, 23, 14],\n",
       "       [16, 18, 18,  8]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndimage.correlate(a_test[0, ...], k_test[0, ...], mode='constant', cval=0) + \\\n",
    "ndimage.correlate(a_test[1, ...], k_test[1, ...], mode='constant', cval=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "tensor_a = torch.FloatTensor(a_test[np.newaxis, ...])\n",
    "tensor_k = torch.FloatTensor(k_test[np.newaxis, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[10., 24., 11.,  3.],\n",
       "          [25., 31., 18.,  6.],\n",
       "          [22., 27., 23., 14.],\n",
       "          [16., 18., 18.,  8.]]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.conv2d(tensor_a, tensor_k, padding=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Image decoder: DDGM\n",
    "\n",
    "First I have to create the operation defined in (1) and (3).  \n",
    "\n",
    "For the convolution, it wasn't clear to me what the padding and stride were, so I assumed that we stick to CNN conventions and make the output shape the same as the input shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def DDGM_convolve(D, S):\n",
    "    \"\"\"\n",
    "    Perform sum_(kl) D^(kl, l) * S^(n, kl, l) as described in the paper. \n",
    "    D: 4D tensor with shape (KL-1, KL, kW, kH)\n",
    "        KL-1: Number of \"slices\" in the previous layer. I.e the out channel. \n",
    "        KL: Number of \"slices\" in the current layer. I.e the in channel.\n",
    "        kW, kH: kernel width and kernel height\n",
    "    \n",
    "    S: 3D tensor with the shape (KL, cW, cH)\n",
    "        KL: Number of 2D slices\n",
    "        iW, iH: code width and code height\n",
    "    \"\"\"\n",
    "    result = None\n",
    "    kernel_size = D.shape[2]\n",
    "    for i in range(D.shape[1]):\n",
    "        D_kl = D[:, i, ...].unsqueeze(1)\n",
    "        S_kl = S[i, ...].unsqueeze(0).unsqueeze(0)\n",
    "        current_sum = F.conv2d(S_kl, D_kl, stride=1, padding=(kernel_size-1)//2)\n",
    "        \n",
    "        if result is None:\n",
    "            result = current_sum\n",
    "        else:\n",
    "            result += current_sum\n",
    "        \n",
    "    return result.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = torch.rand((5, 3, 5, 5), requires_grad=True)\n",
    "S = torch.rand((3, 28, 28))\n",
    "DDGM_convolve(D, S).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Then, I have to create a unpooling layer as defined in (2). I copied an excerpt of it here: \n",
    "\n",
    "\n",
    "For the stochastic unpooling, $S^{(n,k1 ,1)}$ is partitioned into contiguous $px × py$ pooling blocks (analogous to pooling blocks in CNN-based activation maps). Let $ z^{(n,k1 ,1)}_{i, j} \\in \\{0, 1\\}^{px py}$ be a vector\n",
    "of $pxpy − 1$ zeros, and a single one; $z^{(n,k1 ,1)}_{i, j}$ corresponds to pooling block $(i, j)$ in $S^{(n,k1 ,1)}$. \n",
    "\n",
    "\n",
    "The location of the non-zero element of $z^{(n,k1 ,1)}_{i, j} $ identifies the location of the single non-zero element $i,j$\n",
    "in the corresponding pooling block of $S^{(n,k1 ,1)}$. \n",
    "\n",
    "\n",
    "The non-zero element in pooling block $(i,j)$ of $S^{(n,k1 ,1)}$ is set to $\\tilde{S}_{i, j}^{(n,k1,2)}$, i.e., element $(i,j)$ in slice k1 of $\\tilde{S}^{(n,2)}$. \n",
    "\n",
    "Within the prior of the decoder, we impose z(n,k1,1) ∼ Mult(1; 1/(pxpy), . . . , 1/(pxpy)). \n",
    "\n",
    "Both $\\tilde{S}^{(n,2)}$ and $S^{(n,2)}$ are 3D tensors with K1 2D slices; as a result of the unpooling, the 2D slices in the sparse $S^{(n,2)}$ have $pxpy$ times more elements than the corresponding slices in the dense $\\tilde{S}^{(n,2)}$.\n",
    "\n",
    "I tried to illustrate what is I believe is happening below:\n",
    "\n",
    "<img src=\"./images/figure1.jpg\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def unpool(S, prob_vecs):\n",
    "    \"\"\"\n",
    "    Performs stochastic unpooling on S where the location of the non zero element in each pooling block (i, j)\n",
    "    in layer k is defined by z^k (i, j), and z^k, (i, j) \n",
    "    is sampled from a multinomial distribution Mult(1, prob_vecs(k, i, j)).\n",
    "    \n",
    "    Initially, each prob_vec = (1/(pxpy), ... 1/(pxpy)), i.e the prior distribution. \n",
    "    \n",
    "    The shape of each prob_vec must be pool_size**2. Here we are assuming that our pooling blocks\n",
    "    will always be square.\n",
    "    \n",
    "    S: 3D tensor with the shape (KL, cW, cH)\n",
    "    \n",
    "    prob_vecs: 4D tensor with the shape (KL, cW, cH, pool_size**2):\n",
    "        pool size: px * py\n",
    "    \"\"\"\n",
    "    \n",
    "    K, w, h = S.shape\n",
    "    pool_size = int(math.sqrt(prob_vecs.shape[3])) # We are assuming that the pooling blocks are square.\n",
    "    \n",
    "    result = torch.zeros(K, w*pool_size, h*pool_size)\n",
    "    for k in range(K):\n",
    "        for block in itertools.product(range(w), range(h)):\n",
    "            i, j = block\n",
    "            z = torch.zeros(pool_size**2)\n",
    "            idz = torch.multinomial(prob_vecs[k, i, j], 1).item()\n",
    "            z[idz] = S[k, i, j]\n",
    "            result[k, i*pool_size:(i+1)*pool_size, j*pool_size:(j+1)*pool_size] = z.reshape(-1, pool_size)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "With these two functions, we can implement the DDGM. For the data generation layer, I used the trick described in \n",
    "[this article](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73). \n",
    "\n",
    "$$X^{(n)} \\sim \\mathcal{N}(\\tilde{S}^{(n, 1)}, \\alpha_0^{-1} \\textbf{I})$$\n",
    "\n",
    "can be expressed as: \n",
    "\n",
    "$$X^{(n)} = \\tilde{S}^{(n, 1)} + \\alpha_0^{-1} Z $$\n",
    "\n",
    "Where $Z \\sim \\mathcal{N}(\\textbf{0}, \\textbf{I})$ due to the linearity of gaussian distributions. This allows backprop to \"reach\" the dictionary layers as we seperate the random process. I believe this is called a 'reparameterization trick'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "class DDGMDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A 2 layered DDGM decoder, with a stochastic unpooling layer between the two layers,\n",
    "    and a final data generation layer. \n",
    "    \"\"\"\n",
    "    def __init__(self, K2, K1, Nc, d2_kernel, d1_kernel, iW, iH, pool_size=3):\n",
    "        \"\"\"\n",
    "        d2 is of shape (K1, K2, kW, kH)\n",
    "            K1: Number of \"slices\" in layer 1\n",
    "            K2: Number of \"slices\" in layer 2\n",
    "            kW, kH: kernel width, kernel height\n",
    "            \n",
    "        d1 is of shape(Nc, K1, kW, kH):\n",
    "            Nc: Number of channels of the image (1 for grayscale, 3 for rgb)\n",
    "            K1: Number of \"slices\" in layer 1\n",
    "            kW, kH: kernel width, kernel height\n",
    "            \n",
    "        distribution is of shape (iW, iH, pool_size**2):\n",
    "            iW, iW: input width, input height\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self._d2 = nn.Parameter(torch.Tensor(K1, K2, d2_kernel, d2_kernel))\n",
    "        self._d1 = nn.Parameter(torch.Tensor(Nc, K1, d1_kernel, d1_kernel))\n",
    "        self._pool_size = pool_size\n",
    "        self._distribution = torch.ones(K1, iW, iH, pool_size**2) # Uniformly distributed, equal to 1/pxpy, 1/pxpy ...\n",
    "        self._precision = nn.Parameter(torch.rand(1))\n",
    "        self._reset_parameters()\n",
    "        \n",
    "    # Initialization method is taken from the pytorch implementation of CNNs. \n",
    "    # https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#Conv2d\n",
    "    \n",
    "    def _reset_parameters(self):\n",
    "        init.kaiming_uniform_(self._d2, a=math.sqrt(5))\n",
    "        init.kaiming_uniform_(self._d1, a=math.sqrt(5))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        X: code generated from encoder\n",
    "            Shape: (K2, iW, iH)\n",
    "        \"\"\"\n",
    "        x = DDGM_convolve(self._d2, x)\n",
    "        x = unpool(x, self._distribution)\n",
    "        x = DDGM_convolve(self._d1, x)\n",
    "        \n",
    "        # Data Generation\n",
    "        Z = MultivariateNormal(torch.zeros(x.shape[1]*x.shape[2]), torch.eye(x.shape[1]*x.shape[2]))\n",
    "        slices = []\n",
    "        for k in range(x.shape[0]):\n",
    "            mean = x[k, ...].reshape(-1)\n",
    "            covar = (1/self._precision)*(Z.sample())\n",
    "            slice_ = (mean + covar).reshape(x.shape[1], -1)\n",
    "            slices.append(slice_)\n",
    "        return torch.stack(slices)\n",
    "    \n",
    "    def set_distribution(self, distribution):\n",
    "        \"\"\"\n",
    "        Sets the distribution for stochastic unpooling.\n",
    "        \"\"\"\n",
    "        self._distribution = distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "ddgm = DDGMDecoder(K2 = 6, K1 = 4, Nc = 3, d2_kernel=3, d1_kernel=3, iW=10, iH=10)\n",
    "# ddgm(torch.rand(6, 10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "###  Image Encoder: Deep CNN\n",
    "\n",
    "For the encoder, we can employ pytorch's conv2d modules as they perform the convolution that we need, and thus we do not need to define our own convolution method as we did above. However, we do need to create our own pooling function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def pool(C, prob_vecs):\n",
    "    \"\"\"\n",
    "    Performs stochastic pooling on C, where the location of the element chosen in each pooling block (i, j)\n",
    "    in layer kis determined by z^k (i, j), and z(i, j) is sampled from a \n",
    "    multinomial distribution(1, prob_vecs[k, i, j]), and prob_vecs[k, i, j] is derived from \n",
    "    MLP(C^k (i, j)). \n",
    "    \n",
    "    \n",
    "    C is of size (Kl, iW, iH):\n",
    "        KL: Number of 2D \"slices\". \n",
    "        iW, iH: image width, image height. \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    K, w, h = C.shape\n",
    "    pool_size = int(math.sqrt(prob_vecs.shape[-1]))\n",
    "    w_new, h_new = w//pool_size, h//pool_size\n",
    "    result = torch.zeros(K, w_new, h_new)\n",
    "    for k in range(K):\n",
    "        for block in itertools.product(range(w_new), range(h_new)):\n",
    "            i, j = block\n",
    "            \n",
    "            idz = torch.multinomial(prob_vecs[k, i, j], 1).item()\n",
    "            C_block = C[k, i*pool_size:(i+1)*pool_size, j*pool_size:(j+1)*pool_size].reshape(-1)\n",
    "#             prob_vec = F.softmax(mlp(C_block), dim=-1)\n",
    "#             idz = torch.multinomial(prob_vec, 1).item()\n",
    "            \n",
    "            result[k, i, j] = C_block[idz]\n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "class CNNEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A 2 layered CNN encoder, with a stochasting pooling layer between the two layers,\n",
    "    and a final code generation layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, Nc, K1, K2, f1_kernel, f2_kernel, iW, iH, pool_size=3):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        K1: Number of \"slices\" in layer 1\n",
    "        K2: Number of \"slices\" in layer 2\n",
    "        Nc: Number of channels of the image (1 for grayscale, 3 for rgb)\n",
    "        iW, iW: input width, input height\n",
    "        f1_kernel and f2_kernel: Size of the kernels for each of the filter banks.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Conv2d(Nc, K1, kernel_size=f1_kernel, padding=(f1_kernel-1)//2, bias=False)\n",
    "        self.layer2 = nn.Conv2d(K1, K2, kernel_size=f2_kernel, padding=(f2_kernel-1)//2, bias=False)\n",
    "        \n",
    "        # MLP for distribution of stochastic pooling\n",
    "        self.mlp_pool = nn.Sequential(\n",
    "            nn.Linear(pool_size**2, 20), # 20 is randomly chosen here.\n",
    "            nn.Tanh(), \n",
    "            nn.Linear(20, pool_size**2)\n",
    "        )\n",
    "        \n",
    "        # MLP for code generation mean \n",
    "        self.mlp_mean = nn.Sequential(\n",
    "            nn.Linear((iW//pool_size) * (iH//pool_size), 20),\n",
    "            nn.Tanh(), \n",
    "            nn.Linear(20, (iW//pool_size) * (iH//pool_size))\n",
    "        )\n",
    "        \n",
    "        # MLP for code generation covariance \n",
    "        self.mlp_covar = nn.Sequential(\n",
    "            nn.Linear((iW//pool_size) * (iH//pool_size), 20),\n",
    "            nn.Tanh(), \n",
    "            nn.Linear(20, (iW//pool_size) * (iH//pool_size))\n",
    "        )\n",
    "        \n",
    "        self._pool_size = pool_size\n",
    "        \n",
    "    def get_distribution(self, C):\n",
    "        \"\"\"\n",
    "        Gets all the distributions for pooling, i.e the vector z. \n",
    "        This distribution will be reused as the posterior for the decoder's unpooling as well.\n",
    "        \"\"\"\n",
    "        K, w, h = C.shape\n",
    "        pool_size = self._pool_size\n",
    "        w_new, h_new = w // pool_size, h // pool_size\n",
    "      \n",
    "        result = torch.zeros(K, w_new, h_new, self._pool_size**2)\n",
    "        for k in range(K):\n",
    "            for block in itertools.product(range(w_new), range(h_new)):\n",
    "                i, j = block\n",
    "                C_block = C[k, i*pool_size:(i+1)*pool_size, j*pool_size:(j+1)*pool_size].reshape(-1)\n",
    "                prob_vec = F.softmax(self.mlp_pool(C_block), dim=-1)\n",
    "                result[k, i, j, :] = prob_vec\n",
    "                \n",
    "        return result\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Input image\n",
    "            Shape: (Nc, iW, iH)\n",
    "        \"\"\"\n",
    "        \n",
    "        C1 = self.layer1(x.unsqueeze(0))\n",
    "        distribution = self.get_distribution(C1.squeeze(0))\n",
    "        \n",
    "        C1 = pool(C1.squeeze(0), distribution)\n",
    "        C2 = self.layer2(C1.unsqueeze(0))\n",
    "        C2 = C2.squeeze(0)\n",
    "        \n",
    "        # Final Code generation\n",
    "        slices = []\n",
    "        Z = MultivariateNormal(torch.zeros(C2.shape[1]*C2.shape[2]), torch.eye(C2.shape[1]*C2.shape[2]))\n",
    "        for k in range(C2.shape[0]):\n",
    "            mean = self.mlp_mean(C2[k, ...].reshape(-1))\n",
    "            covar = torch.diag(self.mlp_covar(C2[k, ...].reshape(-1))) @ Z.sample()\n",
    "            slice_ = (mean + covar).reshape(C2.shape[1], -1)\n",
    "            slices.append(slice_)\n",
    "    \n",
    "        return torch.stack(slices), distribution # (s, z)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 10, 10]), torch.Size([4, 10, 10, 9]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = CNNEncoder(3, 4, 6, 3, 3, 32, 32)\n",
    "s, z = encoder(torch.rand(3, 32, 32))\n",
    "s.shape, z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting the VAE together\n",
    "\n",
    "Finally, we can piece the encoder and decoder together to form the VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, Nc, K1, K2, f1, f2, d2, d1, iW, iH, pool_size=3):\n",
    "        \"\"\"\n",
    "        Nc: Number of channels of the image. \n",
    "        K1: Number of 2D \"slices\" in layer 1. This is shared for the encoder and decoder.\n",
    "        K2: Number of 2D \"slices\" in layer 2. This is shared for the encoder and decoder. \n",
    "        f1, f2: Kernel size for the first and second filter bank respectively\n",
    "        d2, d1: Kernel size for the second and first dictionary respectively. \n",
    "        iW, iH: width and height of the image. \n",
    "        pool_size: The size of the pooling block. We are assuming that pooling blocks are square.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.encoder = CNNEncoder(Nc, K1, K2, f1, f2, iW, iH, pool_size)\n",
    "        cW, cH = iW // pool_size, iH // pool_size # code width, code height\n",
    "        self.decoder = DDGMDecoder(K2, K1, Nc, d2, d1, cW, cH, pool_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Input image\n",
    "            Shape: (Nc, iW, iH)\n",
    "        \"\"\"\n",
    "        \n",
    "        s, z = self.encoder(x)\n",
    "        self.decoder.set_distribution(z)\n",
    "        return self.decoder(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caption Generator: RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a simple figure to translate section 3.2 into a visual model architecture to help implement the model.\n",
    "\n",
    "<img src=\"./images/model_arch.jpg\" width=600>\n",
    "\n",
    "Therefore, we need 3 layers to perform caption generation:\n",
    "\n",
    "- 2 layered MLP with tanh and softmax activation\n",
    "    - Generates first word from $s^{(n)}$\n",
    "    - Last layer converts hidden state $h_t^{(n)}$ into one hot word vector $y_t^{(n)}$\n",
    "    \n",
    "- Embedding layer\n",
    "    - Converts one hot word vector $y_t^{(n)}$ into word representation $w_t^{(n)}$\n",
    "    \n",
    "- RNN (Can be LTSU or GRU)\n",
    "    - Recursively generates other words until the stop symbol is generated.\n",
    "    \n",
    "\n",
    "\n",
    "For the implementation below, I opted to use the indices instead of a one hot vector since the operations support indices better. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionRNN(nn.Module):\n",
    "    \n",
    "    CAPTION_LIMIT = 30\n",
    "    \n",
    "    def __init__(self, V, M, H, C, stop_index):\n",
    "        \"\"\"\n",
    "        V: Size of the vocabulary\n",
    "        M: Size of embedded vector\n",
    "        H: Number of features for hidden state h\n",
    "        C: Flattened size of input code s\n",
    "        stop_index: The index that represents the end of a sentence.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.mlp_l1 = nn.Linear(in_features=C, out_features=H)\n",
    "        self.mlp_l2 = nn.Linear(in_features=H, out_features=V)\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=M, hidden_size=H, batch_first=True)\n",
    "        self.embedding = nn.Embedding(V, M)\n",
    "        self.stop_index = stop_index\n",
    "        \n",
    "        self.V = V\n",
    "    \n",
    "    def forward(self, x, limit=None):\n",
    "        \"\"\"\n",
    "        x: Unflattened code s.\n",
    "        Limit denotes the maximum number of words we should generate. \n",
    "            Default value can be set by changing CaptionRNN.CAPTION_LIMIT. \n",
    "        \"\"\"\n",
    "        \n",
    "        h1 = F.tanh(self.mlp_l1(x.reshape(-1)))\n",
    "        y1 = torch.multinomial(F.softmax(self.mlp_l2(h1), dim=-1), 1)\n",
    "        \n",
    "        words = [y1]\n",
    "        ht = h1\n",
    "        wt = self.embedding(y1)\n",
    "        yt = -1\n",
    "        \n",
    "        # Prevents the RNN from endlessly creating words\n",
    "        if not limit: \n",
    "            limit = CaptionRNN.CAPTION_LIMIT\n",
    "            \n",
    "        while len(words) < limit and not yt == self.stop_index:\n",
    "            # Output and hidden are the same in this case, so we just get the output. \n",
    "            \n",
    "            ht = self.gru(wt.unsqueeze(0), ht.unsqueeze(0).unsqueeze(0))[0]\n",
    "            ht = ht.squeeze(0).squeeze(0)\n",
    "            yt = torch.multinomial(F.softmax(self.mlp_l2(ht), dim=-1), 1)\n",
    "            words.append(yt)\n",
    "            wt = self.embedding(yt)\n",
    "    \n",
    "        return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = CaptionRNN(V=10, M=20, H=20, C=600, stop_index=None)\n",
    "# test(torch.rand((6, 10, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete model\n",
    "\n",
    "Finally, we put the VAE and CaptionRNN together to create the complete model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAECaption(nn.Module):\n",
    "    \n",
    "    def __init__(self, Nc, K1, K2, f1, f2, d2, d1, iW, iH, V, M, H, stop_index, pool_size=3):\n",
    "        \"\"\"\n",
    "        Nc: Number of channels of the image. \n",
    "        K1: Number of 2D \"slices\" in layer 1. This is shared for the encoder and decoder.\n",
    "        K2: Number of 2D \"slices\" in layer 2. This is shared for the encoder and decoder. \n",
    "        f1, f2: Kernel size for the first and second filter bank respectively\n",
    "        d2, d1: Kernel size for the second and first dictionary respectively. \n",
    "        iW, iH: width and height of the image. \n",
    "        V: Size of the vocabulary\n",
    "        M: Size of embedded vector\n",
    "        H: Number of features for hidden state h\n",
    "        stop_index: The index that indicates end of sentence.\n",
    "        \n",
    "        pool_size: The size of the pooling block. We are assuming that pooling blocks are square.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = CNNEncoder(Nc, K1, K2, f1, f2, iW, iH, pool_size)\n",
    "        cW, cH = iW // pool_size, iH // pool_size # code width, code height\n",
    "        self.decoder = DDGMDecoder(K2, K1, Nc, d2, d1, cW, cH, pool_size)\n",
    "        self.captioner = CaptionRNN(V, M, H, K2*cW*cH, stop_index)\n",
    "        \n",
    "    def forward(self, x, limit=None):\n",
    "        \"\"\"\n",
    "        x: Input image\n",
    "            Shape: (Nc, iW, iH)\n",
    "        Limit denotes the maximum number of words we should generate. \n",
    "            Default value can be set by changing CaptionRNN.CAPTION_LIMIT. \n",
    "        \"\"\"\n",
    "        \n",
    "        s, z = self.encoder(x)\n",
    "        self.decoder.set_distribution(z)\n",
    "        x_reconstructed = self.decoder(s)\n",
    "        caption = self.captioner(s)\n",
    "        return x_reconstructed, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samuelvedrik/venv/lib/python3.7/site-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.4807,  0.0745, -0.3269,  ...,  2.0071, -1.4856,  1.0216],\n",
       "          [-2.9287,  0.2027, -1.5701,  ...,  1.6463,  2.6577, -0.3641],\n",
       "          [-2.4491, -1.9397,  0.2091,  ..., -0.4314,  1.2842,  1.8869],\n",
       "          ...,\n",
       "          [ 2.2065, -1.4675, -1.0623,  ...,  0.2752, -0.0412,  2.9423],\n",
       "          [ 0.2044,  1.6219, -0.3174,  ...,  0.1843,  2.2494, -0.5284],\n",
       "          [ 1.8016,  0.6920,  2.4196,  ..., -1.5634,  0.5712, -0.3255]],\n",
       " \n",
       "         [[ 3.3454, -0.8931, -1.9007,  ..., -1.3881,  1.9869,  0.4065],\n",
       "          [-0.5608, -1.1998,  1.6078,  ..., -1.5067, -0.4759, -1.3706],\n",
       "          [ 1.1602,  1.8228, -2.3619,  ..., -0.0963,  0.8125,  0.9036],\n",
       "          ...,\n",
       "          [ 0.3968,  0.5434, -1.6605,  ..., -0.6974,  1.6825,  2.9489],\n",
       "          [-1.1734,  1.4167,  2.9717,  ...,  1.1532, -0.3852, -0.8545],\n",
       "          [-1.6451,  0.0143, -2.3994,  ...,  1.0019, -1.4454, -0.6245]],\n",
       " \n",
       "         [[ 0.4202, -0.7991,  1.7699,  ...,  2.0978,  0.6428,  0.0891],\n",
       "          [ 0.7188, -0.1137, -0.7332,  ...,  0.5827,  0.6904, -0.2201],\n",
       "          [ 0.8653,  1.2233, -0.1291,  ..., -0.2872, -0.3789,  1.0350],\n",
       "          ...,\n",
       "          [ 0.5221, -0.7646,  0.1808,  ..., -1.2765, -0.7324,  0.1080],\n",
       "          [ 1.1361,  1.7804, -0.2778,  ...,  2.2510,  1.7845,  0.7859],\n",
       "          [-0.1025,  0.3770, -0.1828,  ..., -0.6995,  0.1814, -2.0059]]],\n",
       "        grad_fn=<StackBackward>),\n",
       " [tensor([3]),\n",
       "  tensor([7]),\n",
       "  tensor([7]),\n",
       "  tensor([18]),\n",
       "  tensor([3]),\n",
       "  tensor([9]),\n",
       "  tensor([3]),\n",
       "  tensor([7]),\n",
       "  tensor([9]),\n",
       "  tensor([14]),\n",
       "  tensor([10]),\n",
       "  tensor([6]),\n",
       "  tensor([0]),\n",
       "  tensor([2]),\n",
       "  tensor([14]),\n",
       "  tensor([5]),\n",
       "  tensor([16]),\n",
       "  tensor([0]),\n",
       "  tensor([12]),\n",
       "  tensor([18]),\n",
       "  tensor([12]),\n",
       "  tensor([2]),\n",
       "  tensor([1]),\n",
       "  tensor([15]),\n",
       "  tensor([17]),\n",
       "  tensor([1]),\n",
       "  tensor([17]),\n",
       "  tensor([8]),\n",
       "  tensor([17]),\n",
       "  tensor([6])])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VAECaption(Nc=3, K1 = 4, K2 = 6, f1=3, f2=3, d2=3, d1=3, iW=32, iH=32, V=20, M=30, H=40, stop_index=None)\n",
    "model(torch.rand((3, 32, 32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
